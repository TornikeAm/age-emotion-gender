# -*- coding: utf-8 -*-
"""emotion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UrTJUPEi_llDXfAVzL7fOKUI-srlXrFX
"""

import torch
import torchvision
import warnings
warnings.simplefilter('ignore')
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.nn.functional as F
from zipfile import ZipFile
from torchvision import transforms,datasets,models
import os
import cv2 as cv
import imageio
import glob 
from imgaug import augmenters as iaa 

import matplotlib.pyplot as plt
from loss import FocalLoss





with ZipFile('archive.zip', 'r') as Zip:
  Zip.extractall('Images')

aug = iaa.OneOf([
    iaa.Fliplr(0.5), # horizontally flip 50% of the images
    iaa.Affine(rotate=45),
    iaa.Add(50, per_channel=True),
    iaa.Affine(scale=(0.5, 1.5)),
    iaa.Affine(rotate=((-15,15)))
    
])
path='Images/CK+48/*/*'

def augmentation(augmentor,path):
  print('Augmentation Began')
  print('Augmenting Images...')
  for image in glob.glob(path):	
    for i in range(5):
      aug_name=f"aug{image.split('/',3)[3]}"
      folder="/".join(image.split('/',3)[1:3])
      img=imageio.imread(image)
      aug=augmentor(image=img)
      cv.imwrite(f"{folder}/{i}{aug_name}.png",aug)
      
  print('Done Augmentation')
    
augmentation(aug,path)

root='Images'
data = datasets.ImageFolder(os.path.join(root, 'CK+48'), transform=transforms.Compose([transforms.ToTensor(),transforms.Resize((112,112))]))
train_size = int(0.9 * len(data))
test_size = len(data) - train_size
train_dataset, test_dataset = torch.utils.data.random_split(data, [train_size, test_size])

train_loader=DataLoader(train_dataset,15,shuffle=True)
test_loader=DataLoader(test_dataset,10)

def load_model():
  print("Downloading Model")
  model=models.resnet18(pretrained=True)
  num_ftrs=model.fc.in_features
  model.fc=nn.Linear(num_ftrs,7)
  return model

torch.manual_seed(41)
Emotion_model=load_model()


criterion=FocalLoss()
optimizer=torch.optim.Adam(Emotion_model.parameters(),lr=0.001)

from tqdm import tqdm
def train(model,optim,loss_f,num_of_epochs,path):
  try:
    os.mkdir(path)
  except:
    path=path
  
  min_acc=30

  print('Started Training Emotion Model')  
  for epoch in range(num_of_epochs):
    trn_corr,tst_corr=0,0
    train_losses=[]
    train_acc=[]
    for step,(x_train,y_train) in enumerate(train_loader):
      y_pred=model(x_train)
      y_pred=F.log_softmax(y_pred,dim=1)
      loss=loss_f(y_pred,y_train)

      predicted = torch.max(y_pred.data, 1)[1]
      batch_corr = (predicted == y_train).sum()
      trn_corr += batch_corr

      optim.zero_grad()
      loss.backward()
      optim.step()
    tr_acc=trn_corr.item()*100/(15*step)
    train_losses.append(loss)
     
    
    with torch.no_grad():
      for step, (x_test,y_test) in enumerate(test_loader):
        y_val=model(x_test)
        predicted=torch.max(y_val.data,1)[1]
        tst_corr+=(predicted==y_test).sum()
      val_loss=loss_f(y_val,y_test)
    vl_acc=tst_corr.item()*100/(10*step)

    if min_acc<vl_acc:
      min_acc=vl_acc
      torch.save(model.state_dict(),f"{path}/Emotion_model.pt")
    
     
    print(f'epoch: {epoch:2} train_loss: {loss.item():10.4f}  \
    train_accuracy: {tr_acc:7.3f}% val_loss : {val_loss} val_acc :{vl_acc:7.3f}% ')

train(Emotion_model,optimizer,criterion,3,'models')

