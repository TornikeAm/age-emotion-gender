# -*- coding: utf-8 -*-
"""Age and Gender.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ohOqFqBOVilX6Jb2BraCwZDX176yGJaj
"""



import os 
import glob 
from imgaug import augmenters as iaa 
import warnings
warnings.simplefilter('ignore')
import shutil
from torch.utils.data import Dataset, DataLoader
import mediapipe as mp
import matplotlib.pyplot as plt
import cv2 as cv
from tqdm import tqdm

import torch
import torchvision
import torch.nn.functional as F
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import models

import tarfile
def extract():
  print('Extracting Archives')
  names=['part1.tar.gz','part2.tar.gz','part3.tar.gz']
  for archive in os.listdir(os.getcwd()):
    if archive.split('/',4)[-1] in names:
      tar = tarfile.open(archive, "r:gz")
      tar.extractall()
      tar.close()

extract()



def move_all_images_in_directory():
  folders=['part1','part2','part3']
  try:
    os.mkdir('Data')
  except:
    pass
  for folder in folders:
    for image in glob.glob(f"{folder}/*"):
      shutil.move(image,f"Data/")

move_all_images_in_directory()



try:
  os.mkdir("Face")
except:
  print('allre/ady Created')

class GenderDataset(Dataset):
  def __init__(self,path,path_for_faces):
    self.path=path
    self.path_for_faces=path_for_faces
    self.images=os.listdir(self.path_for_faces)
    self.face_detection = mp.solutions.face_detection.FaceDetection(min_detection_confidence=0.7)
    self.images=os.listdir(self.path_for_faces)
    self.detect_face()	
    self.delete_wrong_labels()
    self.delete_ruined_images()
    


  def detect_face(self):
    for images in glob.glob(self.path):
      print(images.split('/')[-1])
      image = cv.imread(images)
      height, width = image.shape[:2]
      results = self.face_detection.process(cv.cvtColor(image, cv.COLOR_BGR2RGB))
      if results.detections:
        for en, detection in enumerate(results.detections):
          bbox = detection.location_data.relative_bounding_box
          x, y, w, h = bbox.xmin, bbox.ymin, bbox.width, bbox.height
          cropped_face = image[int(y*height):int((y+h)*height), int(x*width):int((x+w)*width)]
          try:
            cv.imwrite(f"Face/{en}_{images.split('/')[-1]}", cropped_face)
          except:
            pass
   
  


  def delete_wrong_labels(self):
    print('Deleting Images With wrong Labels')
    for image in os.listdir('Face'):
      try:
        gender = int(image.split('_')[2])
      except ValueError:
        os.remove(f"Face/{image}")
        continue
    for image in os.listdir('Face'):
      gender=int(image.split('_')[2])
      if gender not in [0,1]:
        os.remove(f'Face/{image}')
      
      
    
  def delete_ruined_images(self):
    print('Deleting Images That Will not open as a Tensor')
    for image in glob.glob('Face/*'):
      try:
        img=cv.imread(image)
        x=img.shape
      except:
        os.remove(image)



    
  def __getitem__(self,index):
    img_path=os.path.join(self.path_for_faces,self.images[index])
    image = cv.imread(img_path)
    image=cv.resize(image,(112,112))
    label=int(self.images[index].split('_',4)[2])
    image=torch.tensor(image).float()
    label=torch.tensor(label)
    return image,label.float()

  def __len__(self):
    return len(self.images)



  
GenderData=GenderDataset('Data/*','Face')



train_size = int(0.9 * len(GenderData))
test_size = len(GenderData) - train_size
train_dataset, test_dataset = torch.utils.data.random_split(GenderData, [train_size, test_size])

train_loader=DataLoader(train_dataset,32,shuffle=True)
test_loader=DataLoader(test_dataset,16)

def load_gender_model():
  model=models.resnet18(pretrained=True)
  num_ftrs=model.fc.in_features
  model.fc=nn.Linear(num_ftrs,2)
  return model

torch.manual_seed(41)
Gender_model=load_gender_model()
Gender_model

criterion=nn.CrossEntropyLoss()
optimizer=torch.optim.Adam(Gender_model.parameters(),lr=0.001)
device=torch.device('cpu')
from tqdm import tqdm
def train(model,optim,loss_f,num_of_epochs,path):
  try:
    os.mkdir(path)
  except:
    path=path
  
  min_acc=30
  print('Gender model Training Started')
  
  for epoch in tqdm(range(num_of_epochs)):
    trn_corr,tst_corr=0,0
    train_losses=[]
    train_acc=[]
    for step,(x_train,y_train) in enumerate(train_loader):
      x_train,y_train=x_train.to(device),y_train.to(device).long()
      x_train = x_train.permute(0, 3, 1, 2)
      y_pred=model(x_train)
      y_pred=F.log_softmax(y_pred,dim=1)
      loss=loss_f(y_pred,y_train)

      predicted = torch.max(y_pred.data, 1)[1]
      batch_corr = (predicted == y_train).sum()
      trn_corr += batch_corr

      optim.zero_grad()
      loss.backward()
      optim.step()
    tr_acc=trn_corr.item()*100/(32*step)
    train_losses.append(loss)
     
    
    with torch.no_grad():
      for step, (x_test,y_test) in enumerate(test_loader):
        x_test,y_train=x_test.to(device),y_train.to(device)
        x_test = x_test.permute(0, 3, 1, 2)
        y_val=model(x_test)
        predicted=torch.max(y_val.data,1)[1]
        tst_corr+=(predicted==y_test).sum()
      val_loss=loss_f(y_val,y_test.long())
    vl_acc=tst_corr.item()*100/(16*step)

    if min_acc<vl_acc:
      min_acc=vl_acc
      torch.save(model.state_dict(),f"{path}/Gendermodel.pt")
    
     
    print(f'epoch: {epoch:2} train_loss: {loss.item():10.4f}  \
    train_accuracy: {tr_acc:7.3f}% val_loss : {val_loss} val_acc :{vl_acc:7.3f}% ')

train(Gender_model,optimizer,criterion,3,'models')

